{
    "id": "b4839cff3b64e2beaacfb7aa7ca2c87d",
    "metadata": {
        "id": "b4839cff3b64e2beaacfb7aa7ca2c87d",
        "url": "https://docs.zenml.io/getting-started/core-concepts",
        "title": "Core Concepts | ZenML - Bridging the gap between ML & Ops",
        "properties": {
            "description": "Discovering the core concepts behind ZenML.",
            "keywords": null,
            "author": null,
            "og:title": "Core Concepts | ZenML - Bridging the gap between ML & Ops",
            "og:description": "Discovering the core concepts behind ZenML.",
            "og:image": "https://docs.zenml.io/~gitbook/ogimage/dqJCRirgIppcDn5SRAnu",
            "twitter:card": "summary_large_image",
            "twitter:title": "Core Concepts | ZenML - Bridging the gap between ML & Ops",
            "twitter:description": "Discovering the core concepts behind ZenML.",
            "twitter:image": "https://docs.zenml.io/~gitbook/ogimage/dqJCRirgIppcDn5SRAnu"
        }
    },
    "content": "`⌘``k`\nGitBook Assistant\nProductResourcesGitHubStart free\nMore\n  * Documentation\n  * Learn\n  * ZenML Pro\n  * Stacks\n  * API Reference\n  * SDK Reference\n  * Changelog\n\n\nGitBook Assistant\nGitBook Assistant\nWorking...Thinking...\nGitBook Assistant\n##### Good afternoon\nI'm here to help you with the docs.\nWhat is this page about?What should I read next?Can you give an example?\n`⌘``i`\nAI Based on your context\nSend\n  * Getting Started\n    *     *     *     *     *     *   * Deploying ZenML\n    *     *     *   * Concepts\n    *     *     *     *     *     *     *     *     *     *     *     *     *     *     *   * Reference\n    *     *     *     *     *     * \n\nPowered by GitBook\n  * 1. Development\n  * 2. Execution\n  * 3. Management\n\n\nWas this helpful?\nGitBook AssistantAsk\n  1. Getting Started\n\n\nDiscovering the core concepts behind ZenML.\nA diagram of core concepts of ZenML OSS\n**ZenML** is a unified, extensible, open-source MLOps framework for creating portable, production-ready **MLOps pipelines**. It's built for data scientists, ML Engineers, and MLOps Developers to collaborate as they develop to production. By extending the battle-tested principles you rely on for classical ML to the new world of AI agents, ZenML serves as one platform to develop, evaluate, and deploy your entire AI portfolio - from decision trees to complex multi-agent systems. In order to achieve this goal, ZenML introduces various concepts for different aspects of ML workflows and AI agent development, and we can categorize these concepts under three different threads:\n**1. Development**\nAs a developer, how do I design my machine learning workflows?\n**2. Execution**\nWhile executing, how do my workflows utilize the large landscape of MLOps tooling/infrastructure?\n**3. Management**\nHow do I establish and maintain a production-grade and efficient solution?\nIf you prefer visual learning, this short video demonstrates the key concepts covered below.\n## \n1. Development\nFirst, let's look at the main concepts that play a role during the development stage of ML workflows and AI agent pipelines with ZenML.\n#### \nStep\nSteps are functions annotated with the `@step` decorator. The easiest one could look like this.\nCopy```\nfrom zenml import step\n@step\ndef step_1() -> str:\n    \"\"\"Returns a string.\"\"\"\n    return \"world\"\n```\n\nThese functions can also have inputs and outputs. For ZenML to work properly, these should preferably be typed.\nCopy```\nfrom zenml import step\n@step(enable_cache=False)\ndef step_2(input_one: str, input_two: str) -> str:\n    \"\"\"Combines the two strings passed in.\"\"\"\n    combined_str = f\"{input_one} {input_two}\"\n    return combined_str\n@step\ndef evaluate_agent_response(prompt: str, test_query: str) -> dict:\n    \"\"\"Evaluates an AI agent's response to a test query.\"\"\"\n    response = call_llm_agent(prompt, test_query)\n    return {\"query\": test_query, \"response\": response, \"quality_score\": 0.95}\n```\n\n#### \nPipelines\nAt its core, ZenML follows a pipeline-based workflow for your projects. A **pipeline** consists of a series of **steps** , organized in any order that makes sense for your use case.\nRepresentation of a pipeline dag.\nAs seen in the image, a step might use the outputs from a previous step and thus must wait until the previous step is completed before starting. This is something you can keep in mind when organizing your steps.\nPipelines and steps are defined in code using Python _decorators_ or _classes_. This is where the core business logic and value of your work live, and you will spend most of your time defining these two things.\nEven though pipelines are simple Python functions, you are only allowed to call steps within this function. The inputs for steps called within a pipeline can either be the outputs of previous steps or alternatively, you can pass in values directly or map them onto pipeline parameters (as long as they're JSON-serializable). Similarly, you can return values from a pipeline that are step outputs as long as they are JSON-serializable.\nCopy```\nfrom zenml import pipeline\n@pipeline\ndef my_pipeline():\n    output_step_one = step_1()\n    step_2(input_one=\"hello\", input_two=output_step_one)\n@pipeline\ndef agent_evaluation_pipeline(query: str = \"What is machine learning?\") -> str:\n    \"\"\"An AI agent evaluation pipeline.\"\"\"\n    prompt = \"You are a helpful assistant. Please answer: {query}\"\n    evaluation_result = evaluate_agent_response(prompt, query)\n    return evaluation_result\n```\n\nExecuting the Pipeline is as easy as calling the function that you decorated with the `@pipeline` decorator.\nCopy```\nif __name__ == \"__main__\":\n    my_pipeline()\n    agent_evaluation_pipeline(query=\"What is an LLM?\")\n```\n\n#### \nArtifacts\nArtifacts represent the data that goes through your steps as inputs and outputs, and they are automatically tracked and stored by ZenML in the artifact store. They are produced by and circulated among steps whenever your step returns an object or a value. This means the data is not passed between steps in memory. Rather, when the execution of a step is completed, they are written to storage, and when a new step gets executed, they are loaded from storage.\nArtifacts can be traditional ML data (datasets, models, metrics) or AI agent components (prompt templates, agent configurations, evaluation results). The same artifact system seamlessly handles both use cases.\nThe serialization and deserialization logic of artifacts is defined by Materializers.\n#### \nModels\nModels are used to represent the outputs of a training process along with all metadata associated with that output. In other words: models in ZenML are more broadly defined as the weights as well as any associated information. This includes traditional ML models (scikit-learn, PyTorch, etc.) and AI agent configurations (prompt templates, tool definitions, multi-agent system architectures). Models are first-class citizens in ZenML and as such viewing and using them is unified and centralized in the ZenML API, client, as well as on the ZenML Pro dashboard.\n#### \nMaterializers\nMaterializers define how artifacts live in between steps. More precisely, they define how data of a particular type can be serialized/deserialized, so that the steps are able to load the input data and store the output data.\nAll materializers use the base abstraction called the `BaseMaterializer` class. While ZenML comes built-in with various implementations of materializers for different datatypes, if you are using a library or a tool that doesn't work with our built-in options, you can write your own custom materializer to ensure that your data can be passed from step to step.\n#### \nParameters & Settings\nWhen we think about steps as functions, we know they receive input in the form of artifacts. We also know that they produce output (in the form of artifacts, stored in the artifact store). But steps also take parameters. The parameters that you pass into the steps are also (helpfully!) stored by ZenML. This helps freeze the iterations of your experimentation workflow in time, so you can return to them exactly as you run them. On top of the parameters that you provide for your steps, you can also use different `Setting`s to configure runtime configurations for your infrastructure and pipelines.\n#### \nModel and model versions\nZenML exposes the concept of a `Model`, which consists of multiple different model versions. A model version represents a unified view of the ML models that are created, tracked, and managed as part of a ZenML project. Model versions link all other entities to a centralized view.\n## \n2. Execution\nOnce you have implemented your workflow by using the concepts described above, you can focus your attention on the execution of the pipeline run.\n#### \nStacks & Components\nWhen you want to execute a pipeline run with ZenML, **Stacks** come into play. A **Stack** is a collection of **stack components** , where each component represents the respective configuration regarding a particular function in your MLOps pipeline, such as pipeline orchestration or deployment systems, artifact repositories and container registries.\nPipelines can be executed in two ways: in **batch mode** (traditional execution through an orchestrator) or in **online mode** (long-running HTTP servers that can be invoked via REST API calls). Deploying pipelines for online mode execution allows you to serve your ML workflows as real-time endpoints, making them accessible for live inference and interactive use cases.\nFor instance, if you take a close look at the default local stack of ZenML, you will see two components that are **required** in every stack in ZenML, namely an _orchestrator_ and an _artifact store_. Additional components like _deployers_ can be added to enable specific functionality such as deploying pipelines as HTTP endpoints.\nZenML running code on the Local Stack.\nKeep in mind that each one of these components is built on top of base abstractions and is completely extensible.\n#### \nOrchestrator\nAn **Orchestrator** is a workhorse that coordinates all the steps to run in a pipeline in batch mode. Since pipelines can be set up with complex combinations of steps with various asynchronous dependencies between them, the orchestrator acts as the component that decides what steps to run and when to run them.\nZenML comes with a default _local orchestrator_ designed to run on your local machine. This is useful, especially during the exploration phase of your project. You don't have to rent a cloud instance just to try out basic things.\n#### \nArtifact Store\nAn **Artifact Store** is a component that houses all data that passes through the pipeline as inputs and outputs. Each artifact that gets stored in the artifact store is tracked and versioned and this allows for extremely useful features like data caching, which speeds up your workflows.\nSimilar to the orchestrator, ZenML comes with a default _local artifact store_ designed to run on your local machine. This is useful, especially during the exploration phase of your project. You don't have to set up a cloud storage system to try out basic things.\n#### \nDeployer\nA **Deployer** is a stack component that manages the deployment of pipelines as long-running HTTP servers useful for online mode execution. Unlike orchestrators that execute pipelines in batch mode, deployers can create and manage persistent services that wrap your pipeline in a web application, usually containerized, allowing it to be invoked through HTTP requests.\nZenML comes with a _Docker deployer_ that can run deployments on your local machine as Docker containers, making it easy to test and develop real-time pipeline endpoints before moving to production infrastructure.\n#### \nFlavor\nZenML provides a dedicated base abstraction for each stack component type. These abstractions are used to develop solutions, called **Flavors** , tailored to specific use cases/tools. With ZenML installed, you get access to a variety of built-in and integrated Flavors for each component type, but users can also leverage the base abstractions to create their own custom flavors.\n#### \nStack Switching\nWhen it comes to production-grade solutions, it is rarely enough to just run your workflow locally without including any cloud infrastructure.\nThanks to the separation between the pipeline code and the stack in ZenML, you can easily switch your stack independently from your code. For instance, all it would take you to switch from an experimental local stack running on your machine to a remote stack that employs a full-fledged cloud infrastructure is a single CLI command.\n#### \nPipeline Snapshot\nA **Pipeline Snapshot** is an immutable snapshot of your pipeline that includes the pipeline DAG, code, configuration, and container images. Snapshots can be run from the server or dashboard, and can also be deployed.\n#### \nPipeline Run\nA **Pipeline Run** is a record of a pipeline execution. When you run a pipeline using an orchestrator, a pipeline run is created tracking information about the execution such as the status, the artifacts and metadata produced by the pipeline and all its steps. When a pipeline is deployed for online mode execution, a pipeline run is similarly created for every HTTP request made to it.\n#### \nDeployment\nA **Deployment** is a running instance of a pipeline deployed as an HTTP endpoint. When you deploy a pipeline using a deployer, it becomes a long-running service that can be invoked through REST API calls. Each HTTP request to a deployment triggers a new pipeline run, creating the same artifacts and metadata tracking as traditional batch pipeline executions. This enables real-time inference, interactive ML workflows, and seamless integration with web applications and external services.\n## \n3. Management\nIn order to benefit from the aforementioned core concepts to their fullest extent, it is essential to deploy and manage a production-grade environment that interacts with your ZenML installation.\n#### \nZenML Server\nTo use _stack components_ that are running remotely on a cloud infrastructure, you need to deploy a **ZenML Server** so it can communicate with these stack components and run your pipelines. The server is also responsible for managing ZenML business entities like pipelines, steps, models, etc.\nVisualization of the relationship between code and infrastructure.\n#### \nServer Deployment\nIn order to benefit from the advantages of using a deployed ZenML server, you can either choose to use the **ZenML Pro SaaS offering****,** which provides a control plane for you to create managed instances of ZenML servers, or deploy it in your self-hosted environment.\n#### \nMetadata Tracking\nOn top of the communication with the stack components, the **ZenML Server** also keeps track of all the bits of metadata around a pipeline run. With a ZenML server, you are able to access all of your previous experiments with the associated details. This is extremely helpful in troubleshooting.\n#### \nSecrets\nThe **ZenML Server** also acts as a centralized secrets store that safely and securely stores sensitive data, such as credentials used to access the services that are part of your stack. It can be configured to use a variety of different backends for this purpose, such as the AWS Secrets Manager, GCP Secret Manager, Azure Key Vault, and Hashicorp Vault.\nSecrets are sensitive data that you don't want to store in your code or configure alongside your stacks and pipelines. ZenML includes a centralized secrets store that you can use to store and access your secrets securely.\n#### \nCollaboration\nCollaboration is a crucial aspect of any MLOps team as they often need to bring together individuals with diverse skills and expertise to create a cohesive and effective workflow for machine learning projects and AI agent development. A successful MLOps team requires seamless collaboration between data scientists, engineers, and DevOps professionals to develop, train, deploy, and maintain both traditional ML models and AI agent systems.\nWith a deployed **ZenML Server** , users have the ability to create their own teams and project structures. They can easily share pipelines, runs, stacks, and other resources, streamlining the workflow and promoting teamwork across the entire AI development lifecycle.\n#### \nDashboard\nThe **ZenML Dashboard** also communicates with **the ZenML Server** to visualize your _pipelines_ , _stacks_ , and _stack components_. The dashboard serves as a visual interface to showcase collaboration with ZenML. You can invite _users_ and share your stacks with them.\nWhen you start working with ZenML, you'll start with a local ZenML setup, and when you want to transition, you will need to deploy ZenML. Don't worry though, there is a one-click way to do it, which we'll learn about later.\n#### \nVS Code Extension\nZenML also provides a VS Code extension that allows you to interact with your ZenML stacks, runs, and server directly from your VS Code editor. If you're working on code in your editor, you can easily switch and inspect the stacks you're using, delete and inspect pipelines as well as even switch stacks.\nPreviousYour First AI PipelineNextSystem Architecture\nLast updated 1 month ago\nWas this helpful?\nThis site uses cookies to deliver its service and to analyze traffic. By browsing this site, you accept the privacy policy.\nAcceptReject\n",
    "summary": "## TL;DR Summary of ZenML Documentation\n\n### 1. Getting Started\n**ZenML** is an open-source MLOps framework designed for creating production-ready ML pipelines. It facilitates collaboration among data scientists and ML engineers through three main threads:\n- **Development**: Design ML workflows using `@step` decorators for functions and define pipelines with Python decorators.\n- **Execution**: Utilize **Stacks** (collections of components) for running pipelines in batch or online modes, with orchestrators managing execution and artifact stores tracking data.\n- **Management**: Deploy a **ZenML Server** for remote stack components, manage metadata, and securely store secrets.\n\n### Key Concepts\n- **Steps**: Functions that process data.\n- **Pipelines**: Series of steps organized for execution.\n- **Artifacts**: Data tracked through steps.\n- **Materializers**: Handle data serialization/deserialization.\n- **Deployments**: HTTP endpoints for real-time inference.\n\n### Collaboration\nZenML promotes teamwork through shared resources and a dashboard for visual management. A VS Code extension enhances interaction with ZenML components directly from the editor.",
    "content_quality_score": null,
    "child_urls": [
        "https://docs.zenml.io/",
        "https://zenml.io",
        "https://zenml.io/slack",
        "https://cloud.zenml.io/signup",
        "https://docs.zenml.io/user-guides",
        "https://docs.zenml.io/pro",
        "https://docs.zenml.io/stacks",
        "https://docs.zenml.io/api-reference",
        "https://docs.zenml.io/sdk-reference",
        "https://docs.zenml.io/changelog",
        "https://docs.zenml.io/getting-started/installation",
        "https://docs.zenml.io/getting-started/hello-world",
        "https://docs.zenml.io/getting-started/your-first-ai-pipeline",
        "https://docs.zenml.io/getting-started/core-concepts",
        "https://docs.zenml.io/getting-started/system-architectures",
        "https://docs.zenml.io/deploying-zenml/deploying-zenml",
        "https://docs.zenml.io/deploying-zenml/connecting-to-zenml",
        "https://docs.zenml.io/deploying-zenml/upgrade-zenml-server",
        "https://docs.zenml.io/concepts/steps_and_pipelines",
        "https://docs.zenml.io/concepts/artifacts",
        "https://docs.zenml.io/concepts/stack_components",
        "https://docs.zenml.io/concepts/service_connectors",
        "https://docs.zenml.io/concepts/snapshots",
        "https://docs.zenml.io/concepts/deployment",
        "https://docs.zenml.io/concepts/containerization",
        "https://docs.zenml.io/concepts/code-repositories",
        "https://docs.zenml.io/concepts/secrets",
        "https://docs.zenml.io/concepts/environment-variables",
        "https://docs.zenml.io/concepts/tags",
        "https://docs.zenml.io/concepts/metadata",
        "https://docs.zenml.io/concepts/models",
        "https://docs.zenml.io/concepts/dashboard-features",
        "https://docs.zenml.io/concepts/templates",
        "https://docs.zenml.io/reference/community-and-content",
        "https://docs.zenml.io/reference/environment-variables",
        "https://docs.zenml.io/reference/llms-txt",
        "https://docs.zenml.io/reference/faq",
        "https://docs.zenml.io/reference/global-settings",
        "https://docs.zenml.io/reference/legacy-docs",
        "https://docs.zenml.io/getting-started",
        "https://docs.zenml.io/concepts/artifacts/materializers",
        "https://zenml.io/pro",
        "https://docs.zenml.io/user-guides/production-guide/deploying-zenml",
        "https://docs.zenml.io/deploying-zenml/deploying-zenml/secret-management",
        "https://www.zenml.io/privacy-policy",
        "https://github.com/zenml-io/zenml",
        "https://www.gitbook.com/",
        "https://marketplace.visualstudio.com/items?itemname=ZenML.zenml-vscode"
    ]
}