{
    "id": "f80e0afc0fd4ebef3264d7eefca6dcd6",
    "metadata": {
        "id": "f80e0afc0fd4ebef3264d7eefca6dcd6",
        "url": "https://docs.zenml.io/concepts/deployment",
        "title": "Pipeline Deployments | ZenML - Bridging the gap between ML & Ops",
        "properties": {
            "description": "Deploy pipelines as HTTP services for real-time execution",
            "keywords": null,
            "author": null,
            "og:title": "Pipeline Deployments | ZenML - Bridging the gap between ML & Ops",
            "og:description": "Deploy pipelines as HTTP services for real-time execution",
            "og:image": "https://docs.zenml.io/~gitbook/ogimage/6XZsAzUOP7SjOqdzDoqi",
            "twitter:card": "summary_large_image",
            "twitter:title": "Pipeline Deployments | ZenML - Bridging the gap between ML & Ops",
            "twitter:description": "Deploy pipelines as HTTP services for real-time execution",
            "twitter:image": "https://docs.zenml.io/~gitbook/ogimage/6XZsAzUOP7SjOqdzDoqi"
        }
    },
    "content": "`Ctrl``k`\nGitBook AssistantAsk\nProductResourcesGitHubStart free\nMore\n  * Documentation\n  * Learn\n  * ZenML Pro\n  * Stacks\n  * API Reference\n  * SDK Reference\n  * Changelog\n\n\nGitBook Assistant\nGitBook Assistant\nWorking...Thinking...\nGitBook Assistant\n##### Good afternoon\nI'm here to help you with the docs.\nWhat is this page about?What should I read next?Can you give an example?\n`Ctrl``i`\nAI Based on your context\nSend\n  * Getting Started\n    *     *     *     *     *     *   * Deploying ZenML\n    *     *     *   * Concepts\n    *     *     *     *     *     *       * Deployment Settings\n    *     *     *     *     *     *     *     *     *   * Reference\n    *     *     *     *     *     * \n\nPowered by GitBook\n  * What is a Pipeline Deployment?\n  * Common Use Cases\n  * Traditional Model Serving vs. Deployed Pipelines\n  * How Deployments Work\n  * Deployment Lifecycle\n  * Managing Deployments\n  * Deployable Pipeline Requirements\n  * Pipeline Input Parameters\n  * Pipeline Outputs\n  * Deployment Authentication\n  * Deployment Initialization, Cleanup and State\n  * Deployment Configuration\n  * Best Practices\n  * Conclusion\n\n\nWas this helpful?\nGitBook AssistantAsk\n  1. Concepts\n\n\nDeploy pipelines as HTTP services for real-time execution\nPipeline deployment allows you to run ZenML pipelines as long-running HTTP services for real-time execution, rather than traditional batch mode execution. This enables you to invoke pipelines through HTTP requests and receive immediate responses.\n## \nWhat is a Pipeline Deployment?\nA pipeline deployment is a long-running HTTP server that wraps your pipeline for real-time, request-response interactions. While traditional (batch) pipeline execution (via orchestrators) is ideal for scheduled batch processing, data transformations, and offline training workflows, deployments are designed for scenarios where you need immediate responses - like serving predictions to a web app, processing user requests, or powering interactive AI agents. Deployments create persistent services that stay running and can handle multiple concurrent requests through HTTP endpoints.\nWhen you deploy a pipeline, ZenML creates an HTTP server (called a **Deployment**) that can execute your pipeline multiple times in parallel by invoking HTTP endpoints.\n## \nCommon Use Cases\nPipeline deployments are ideal for scenarios requiring real-time, on-demand execution of ML workflows:\n**Online ML Inference** : Deploy trained models as HTTP services for real-time predictions, such as fraud detection in payment systems, recommendation engines for e-commerce, or image classification APIs. Pipeline deployments handle feature preprocessing, model loading, and prediction logic while managing concurrent requests efficiently.\n**LLM Agent Workflows** : Build intelligent agents that combine multiple AI capabilities like intent analysis, retrieval-augmented generation (RAG), and response synthesis. These deployments can power chatbots, customer support systems, or document analysis services that require multi-step reasoning and context retrieval. See the Agent Outer Loop and Deploying Agents examples for practical implementations.\n**Real-time Data Processing** : Process streaming events or user interactions that require immediate analysis and response, such as real-time analytics dashboards, anomaly detection systems, or personalization engines.\n**Multi-step Business Workflows** : Orchestrate complex processes involving multiple AI/ML components, like document processing pipelines that combine OCR, entity extraction, sentiment analysis, and classification into a single deployable service.\n## \nTraditional Model Serving vs. Deployed Pipelines\nIf you're reaching for tools like Seldon or KServe, consider this: deployed pipelines give you all the core serving primitives, plus the power of a full application runtime.\n  * Equivalent functionality: A pipeline handles the end-to-end inference path out of the box â€” request validation, feature pre-processing, model loading and inference, post-processing, and response shaping.\n  * More flexible: Deployed pipelines are unopinionated, so you can layer in retrieval, guardrails, rules, A/B routing, canary logic, human-in-the-loop, or any custom orchestration. You're not constrained by a model-server template.\n  * More customizable: The deployment is a real ASGI app. Tailor endpoints, authentication, authorization, rate limiting, structured logging, tracing, correlation IDs, or SSO/OIDC â€” all with first-class middleware and framework-level hooks.\n  * More features: Serve single-page apps alongside the API. Ship admin/ops dashboards, experiment playgrounds, model cards, or customer-facing UIs from the very same deployment for tighter operational feedback loops.\n\n\nThis approach aligns better with production realities: inference is rarely \"just call a model.\" There are policies, data dependencies, and integrations that need a programmable, evolvable surface. Deployed pipelines give you that without sacrificing the convenience of a managed deployer and a clean HTTP contract.\nDeprecation notice: ZenML is phasing out the Model Deployer stack components in favor of pipeline deployments. Pipeline deployments are the strategic direction for real-time serving: they are more dynamic, more extensible, and offer deeper integration points with your security, observability, and product requirements. Existing model deployers will continue to function during the transition period, but new investments will focus on pipeline deployments.\n## \nHow Deployments Work\nTo deploy a pipeline or snapshot, a **Deployer** stack component needs to be in your active stack. You can use the default stack, which has a default local deployer that will deploy the pipeline directly on your local machine as a background process:\nCopy```\nzenml stack set default\n```\n\nor set up a new stack with a deployer in it:\nCopy```\nzenml deployer register <DEPLOYER-NAME> --flavor=<DEPLOYER-FLAVOR>\nzenml stack update -d <DEPLOYER-NAME>\n```\n\nThe **Deployer** stack component manages the deployment of pipelines as long-running HTTP servers. It integrates with a specific infrastructure back-end like Docker, AWS App Runner, GCP Cloud Run etc., in order to implement the following functionalities:\n  * Creating and managing persistent containerized services\n  * Exposing HTTP endpoints for pipeline invocation\n  * Managing the lifecycle of deployments (creation, updates, deletion)\n  * Providing connection information and management commands\n\n\nThe **Deployer** and **Model Deployer** represent distinct stack components with slightly overlapping responsibilities. The **Deployer** component orchestrates the deployment of arbitrary pipelines as persistent HTTP services, while the **Model Deployer** component focuses exclusively on the deployment and management of ML models for real-time inference scenarios.\nThe **Deployer** component can easily accommodate ML model deployment through deploying ML inference pipelines. This approach provides enhanced flexibility for implementing custom business logic and preprocessing workflows around the deployed model artifacts. Conversely, specialized **Model Deployer** integrations may offer optimized deployment strategies, superior performance characteristics, and resource utilization efficiencies that exceed the capabilities of general-purpose pipeline deployments.\nWhen deciding which component to use, consider the trade-offs between how much control you need over the deployment process and how much you want to offload to a particular integration specialized for ML model serving.\nWith a **Deployer** stack component in your active stack, a pipeline or snapshot can be deployed using the ZenML CLI:\nCopy```\n# Deploy the pipeline `weather_pipeline` in the `weather_agent` module as a\n# deployment named `my_deployment`\nzenml pipeline deploy weather_agent.weather_pipeline --name my_deployment\n# Deploy a snapshot named `weather_agent_snapshot` as a deployment named\n# `my_deployment`\nzenml pipeline snapshot deploy weather_agent_snapshot --deployment my_deployment\n```\n\nTo deploy a pipeline using the ZenML SDK:\nCopy```\nfrom zenml.pipeline import pipeline\n@pipeline\ndef weather_agent(city: str = \"Paris\", temperature: float = 20) -> str:\n    return process_weather(city=city, temperature=temperature)\n# Deploy the pipeline `weather_agent` as a deployment named `my_deployment`\ndeployment = weather_agent.deploy(deployment_name=\"my_deployment\")\nprint(f\"Deployment URL: {deployment.url}\")\n```\n\nIt is also possible to deploy snapshots programmatically:\nCopy```\nfrom zenml.client import Client\nclient = Client()\nsnapshot = client.get_snapshot(snapshot_name_or_id=\"weather_agent_snapshot\")\n# Deploy the snapshot `weather_agent_snapshot` as a deployment named\n# `my_deployment`\ndeployment = client.provision_deployment(\n    name_id_or_prefix=\"my_deployment\",\n    snapshot_id=snapshot.id,\n)\nprint(f\"Deployment URL: {deployment.url}\")\n```\n\nOnce deployed, a pipeline can be invoked through the URL exposed by the deployment. Every invocation of the deployment will create a new pipeline run.\nThe ZenML CLI provides a convenient command to invoke a deployment:\nCopy```\nzenml deployment invoke my_deployment --city=\"London\" --temperature=20\n```\n\nwhich is the equivalent of the following HTTP request:\nCopy```\ncurl -X POST http://localhost:8000/invoke \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"parameters\": {\"city\": \"London\", \"temperature\": 20}}'\n```\n\n## \nDeployment Lifecycle\nOnce a Deployment is created, it is tied to the specific **Deployer** stack component that was used to provision it and can be managed independently of the active stack as a standalone entity with its own lifecycle.\nA Deployment contains the following key information:\n  * `**name**`: Unique deployment name within the project\n  * `**url**`: HTTP endpoint URL where the deployment can be accessed\n  * `**status**`: Current deployment status. This can take one of the following values`DeploymentStatus` enum values:\n    * `**RUNNING**`: The deployment is running and accepting HTTP requests\n    * `**ABSENT**`: The deployment is not currently provisioned\n    * `**PENDING**`: The deployment is currently undergoing some operation (e.g. being created, updated or deleted)\n    * `**ERROR**`: The deployment is in an error state. When in this state, more information about the error can be found in the ZenML logs, the Deployment`metadata` field or in the Deployment logs.\n    * `**UNKNOWN**`: The deployment is in an unknown state\n  * `**metadata**`: Deployer-specific metadata describing the deployment's operational state\n\n\n### \nManaging Deployments\nTo list all the deployments managed in your project by all the available Deployers:\nCopy```\nzenml deployment list\n```\n\nThis shows a table with deployment details:\nCopy```\nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚         NAME         â”‚ PIPELINE               â”‚ SNAPSHOT             â”‚ URL                   â”‚ STATUS    â”‚ STACK           â”‚ OWNER           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  zenpulse-endpoint   â”‚ zenpulse_agent         â”‚                      â”‚ http://localhost:8000 â”‚ âš™ RUNNING â”‚ aws-stack       â”‚ hamza@zenml.io  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ docker-weather-agent â”‚ weather_agent_pipeline â”‚ docker-weather-agent â”‚ http://localhost:8000 â”‚ âš™ RUNNING â”‚ docker-deployer â”‚ stefan@zenml.io â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚    weather_agent     â”‚ weather_agent          â”‚                      â”‚ http://localhost:8001 â”‚ âš™ RUNNING â”‚ docker-deployer â”‚ stefan@zenml.io â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n```\n\nDetailed information about a specific deployment can be obtained with the following command:\nCopy```\nzenml deployment describe weather_agent\n```\n\nThis provides comprehensive deployment details, including its state and access information:\nCopy```\nðŸš€ Deployment: weather_agent is: RUNNING âš™\nPipeline: weather_agent\nSnapshot: 0866c821-d73f-456d-a98d-9aa82f41282e\nStack: docker-deployer\nðŸ“¡ Connection Information:\nEndpoint URL: http://localhost:8001\nSwagger URL: http://localhost:8001/docs\nCLI Command Example:\n  zenml deployment invoke weather_agent --city=\"London\"\ncURL Example:\n  curl -X POST http://localhost:8001/invoke \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n      \"parameters\": {\n        \"city\": \"London\"\n      }\n    }'\nâš™ï¸  Management Commands\nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ zenml deployment logs weather_agent -f     â”‚ Follow deployment logs in real-time                 â”‚\nâ”‚ zenml deployment describe weather_agent    â”‚ Show detailed deployment information                â”‚\nâ”‚ zenml deployment deprovision weather_agent â”‚ Deprovision this deployment and keep a record of it â”‚\nâ”‚ zenml deployment delete weather_agent      â”‚ Deprovision and delete this deployment              â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n```\n\nAdditional information regarding the deployment can be shown with the same command:\n  * schema information about the deployment's input and output\n  * backend-specific metadata information about the deployment\n  * authentication information, if present\n\n\nDeploying or redeploying a pipeline or snapshot on top of an existing deployment will update the deployment in place:\nCopy```\n# Update the existing deployment named `my_deployment` with a new pipeline\n# code version\nzenml pipeline deploy weather_agent.weather_pipeline --name my_deployment --update\n# Update the existing deployment named `my_deployment` with a new snapshot\n# named `other_weather_agent_snapshot`\nzenml deployment provision my_deployment --snapshot other_weather_agent_snapshot\n```\n\n**Deployment update checks and limitations**\n  * Updating a deployment owned by a different user requires additional confirmation. This is to avoid unintentionally updating someone else's deployment.\n  * An existing deployment cannot be updated using a stack different from the one it was originally deployed with.\n  * A pipeline snapshot can only have one deployment running at a time. You cannot deploy the same snapshot multiple times. You either have to delete the existing deployment and deploy the snapshot again or create a different snapshot.\n\n\nDeprovisioning and deleting a deployment are two different operations. Deprovisioning a deployment keeps a record of it in the ZenML database so that it can be easily restored later if needed. Deleting a deployment completely removes it from the ZenML store:\nCopy```\n# Deprovision the deployment named `my_deployment`\nzenml deployment deprovision my_deployment\n# Re-provision the deployment named `my_deployment` with the same configuration as before\nzenml deployment provision my_deployment\n# Deprovision and delete the deployment named `my_deployment`\nzenml deployment delete my_deployment\n```\n\n**Deployer deletion**\nA Deployer stack component cannot be deleted as long as there is at least one deployment managed by it that is not in an `ABSENT` state. To delete a Deployer stack component, you need to first deprovision or delete all the deployments managed by it. If some deployments are stuck in an `ERROR` state, you can use the `--force` flag to delete them without the need to deprovision them first, but be aware that this may leave some infrastructure resources orphaned.\nThe server logs of a deployment can be accessed with the following command:\nCopy```\nzenml deployment logs my_deployment\n```\n\n## \nDeployable Pipeline Requirements\nWhile any pipeline can technically be deployed, following these guidelines ensures practical usability:\n### \nPipeline Input Parameters\nPipelines should accept explicit parameters to enable dynamic invocation:\nCopy```\n@pipeline\ndef weather_agent(city: str = \"Paris\", temperature: float = 20) -> str:\n    return process_weather(city=city, temperature=temperature)\n```\n\n**Input Parameter Requirements:**\n  * All pipeline input parameters must have default values. This is a current limitation of the deployment mechanism.\n  * Input parameters must use JSON-serializable data types (`int`, `float`, `str`, `bool`, `list`, `dict`, `tuple`, Pydantic models). Other data types are not currently supported and will result in an error when deploying the pipeline.\n  * Pipeline input parameter names must match step parameter names. E.g. if the pipeline has an input parameter named `city` that is passed to a step input argument, that step argument must also be named `city`.\n\n\nWhen deployed, the example pipeline above can be invoked:\n  * with a CLI command like the following:\n\n\nCopy```\nzenml deployment invoke my_pipeline --city=Paris --temperature=20\n```\n\n  * or with an HTTP request like the following:\n\n\nCopy```\ncurl -X POST http://localhost:8000/invoke \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"parameters\": {\"city\": \"Paris\", \"temperature\": 20}}'\n```\n\nPipeline input parameters behave differently when pipelines are deployed than when they are run as a batch job. When running a parameterized pipeline, its input parameters are evaluated before the pipeline run even starts and can be used to configure the structure of the pipeline DAG. When invoking a deployment, the input parameters do not have an effect on the pipeline DAG structure, so a pipeline like the following will not work as expected:\nCopy```\n@pipeline\ndef switcher(\n    mode: str = \"analyze\",\n    city: str = \"Paris\",\n    topic: str = \"ML\",\n) -> str:\n    return (\n        analyze(city) if mode == \"analyze\" else generate(topic)\n    )  # this will always use the \"analyze\" step when deploying the pipeline\n```\n\n### \nPipeline Outputs\nPipelines should return meaningful values for useful HTTP responses:\nCopy```\n@step\ndef process_weather(city: str, temperature: float) -> Annotated[str, \"weather_analysis\"]:\n    return f\"The weather in {city} is {temperature} degrees Celsius.\"\n@pipeline\ndef weather_agent(city: str = \"Paris\", temperature: float = 20) -> str:\n    weather_analysis = process_weather(city=city, temperature=temperature)\n    return weather_analysis\n```\n\n**Output Requirements:**\n  * Return values must be step outputs.\n  * Return values must be JSON-serializable (`int`, `float`, `str`, `bool`, `list`, `dict`, `tuple`, Pydantic models). Other data types are not currently supported and will result in an error when deploying the pipeline.\n  * The names of the step output artifacts determine the response structure (see example below)\n  * For clashing output names, the naming convention used to differentiate them is `<step_name>.<output_name>`\n\n\nInvoking a deployment of this pipeline will return the response below. Note how the `outputs` field contains the value returned by the `process_weather` step and the name of the output artifact is used as the key.\nCopy```\n{\n    \"success\": true,\n    \"outputs\": {\n        \"weather_analysis\": \"The weather in Utopia is 25 degrees Celsius\"\n    },\n    \"execution_time\": 8.160255432128906,\n    \"metadata\": {\n        \"deployment_id\": \"e0b34be2-d743-4686-a45b-c12e81627bbe\",\n        \"deployment_name\": \"weather_agent\",\n        \"snapshot_id\": \"0866c821-d73f-456d-a98d-9aa82f41282e\",\n        \"snapshot_name\": null,\n        \"pipeline_name\": \"weather_agent\",\n        \"run_id\": \"f2e9a3a7-afa3-459e-a970-8558358cf1fb\",\n        \"run_name\": \"weather_agent-2025_09_29-14_09_55_726165\",\n        \"parameters_used\": {\n            \"city\": \"Utopia\",\n            \"temperature\": 25\n        }\n    },\n    \"error\": null\n}\n```\n\n### \nDeployment Authentication\nA rudimentary form of HTTP Basic authentication can be enabled for deployments by configuring one of two deployer configuration options:\n  * `generate_auth_key`: set to `True` to automatically generate a shared secret key for the deployment. This is not set by default.\n  * `auth_key`: configure the shared secret key manually.\n\n\nCopy```\n@pipeline(\n    settings={\n        \"deployer\": {\n            \"generate_auth_key\": True,\n        }\n    }\n)\ndef weather_agent(city: str = \"Paris\", temperature: float = 20) -> str:\n    return process_weather(city=city, temperature=temperature)\n```\n\nDeploying the above pipeline automatically generates and returns a key that will be required in the `Authorization` header of HTTP requests made to the deployment:\nCopy```\ncurl -X POST http://localhost:8000/invoke \\\n  -H \"Authorization: Bearer <GENERATED_AUTH_KEY>\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"parameters\": {\"city\": \"Paris\", \"temperature\": 20}}'\n```\n\n## \nDeployment Initialization, Cleanup and State\nIt often happens that the HTTP requests made to the same deployment share some type of initialization or cleanup or need to share the same global state or. For example:\n  * a machine learning model needs to be loaded in memory, initialized and then shared between all the HTTP requests made to the deployment in order to be used by the deployed pipeline to make predictions\n  * a database client must be initialized and shared across all the HTTP requests made to the deployment in order to read and write data\n\n\nTo achieve this, it is possible to configure custom initialization and cleanup hooks for the pipeline being deployed:\nCopy```\ndef init_llm(model_name: str):\n    # Initialize and store the LLM in memory when the deployment is started, to\n    # be shared by all the HTTP requests made to the deployment\n    return LLM(model_name=model_name)\ndef cleanup_llm(llm: LLM):\n    # Cleanup the LLM when the deployment is stopped\n    llm.cleanup()\n@step\ndef process_weather(city: str, temperature: float) -> Annotated[str, \"weather_analysis\"]:\n    step_context = get_step_context()\n    # The value returned by the on_init hook is stored in the pipeline state\n    llm = step_context.pipeline_state\n    return generate_llm_response(llm, city, temperature)\n@pipeline(\n    on_init=init_llm,\n    on_cleanup=cleanup_llm,\n)\ndef weather_agent(city: str = \"Paris\", temperature: float = 20) -> str:\n    return process_weather(city=city, temperature=temperature)\nweather_agent_deployment = weather_agent.with_options(\n    on_init_kwargs={\"model_name\": \"gpt-4o\"},\n).deploy(deployment_name=\"my_deployment\")\n```\n\nThe following happens when the pipeline is deployed and then later invoked:\n  1. The on_init hook is executed only once, when the deployment is started\n  2. The value returned by the on_init hook is stored in memory in the deployment and can be accessed by pipeline steps using the `pipeline_state` property of the step context\n  3. The on_cleanup hook is executed only once, when the deployment is stopped\n\n\nThis mechanism can be used to initialize and share global state between all the HTTP requests made to the deployment or to execute long-running initialization or cleanup operations when the deployment is started or stopped rather than on each HTTP request.\n## \nDeployment Configuration\nThe deployer settings cover aspects of the pipeline deployment process and specific back-end infrastructure used to provision and manage the resources required to run the deployment servers. Independently of that, `DeploymentSettings` can be used to fully customize all aspects pertaining to the deployment ASGI application itself, including:\n  * HTTP endpoints\n  * middleware\n  * secure headers\n  * CORS settings\n  * mounting and serving static files to support deploying single-page applications alongside the pipeline\n  * for more advanced cases, even the ASGI framework (e.g. FastAPI, Django, Flask, Falcon, Quart, BlackSheep, etc.) and its configuration can be customized\n\n\nExample:\nCopy```\nfrom zenml.config import DeploymentSettings, EndpointSpec, EndpointMethod\nfrom zenml import pipeline\nasync def custom_health_check() -> Dict[str, Any]:\n    from zenml.client import Client\n    client = Client()\n    return {\n        \"status\": \"healthy\",\n        \"info\": client.zen_store.get_store_info().model_dump(),\n    }\n@pipeline(settings={\"deployment\": DeploymentSettings(\n    custom_endpoints=[\n        EndpointSpec(\n            path=\"/health\",\n            method=EndpointMethod.GET,\n            handler=custom_health_check,\n            auth_required=False,\n        ),\n    ],\n)})\ndef my_pipeline():\n    ...\n```\n\nFor more detailed information on deployment options, see the deployment settings guide.\n## \nBest Practices\n  1. **Design for Parameters** : Structure your pipelines to accept meaningful parameters that control behavior\n  2. **Provide Default Values** : Ensure all parameters have sensible defaults\n  3. **Return Useful Data** : Design pipeline outputs to provide meaningful responses\n  4. **Use Type Annotations** : Leverage Pydantic models for complex parameter types\n  5. **Use Global Initialization and State** : Use the `on_init` and `on_cleanup` hooks along with the `pipeline_state` step context property to initialize and share global state between all the HTTP requests made to the deployment. Also use these hooks to execute long-running initialization or cleanup operations when the deployment is started or stopped rather than on each HTTP request.\n  6. **Handle Errors Gracefully** : Implement proper error handling in your steps\n  7. **Test Locally First** : Validate your deployable pipeline locally before deploying to production\n\n\n## \nConclusion\nPipeline deployment transforms ZenML pipelines from batch processing workflows into real-time services. By following the guidelines for deployable pipelines and understanding the deployment lifecycle, you can create robust, scalable ML services that integrate seamlessly with web applications and real-time systems.\nSee also:\n  * Steps & Pipelines - Core building blocks\n  * Deployer Stack Component - The stack component that manages the deployment of pipelines as long-running HTTP servers\n\n\nPreviousPipeline SnapshotsNextDeployment Settings\nLast updated 1 month ago\nWas this helpful?\n",
    "summary": "## TL;DR Summary of ZenML Pipeline Deployment Documentation\n\n### What is a Pipeline Deployment?\n- A long-running HTTP server that allows real-time execution of ZenML pipelines, enabling immediate responses to HTTP requests.\n\n### Common Use Cases\n- **Online ML Inference**: Real-time predictions (e.g., fraud detection).\n- **LLM Agent Workflows**: Intelligent agents for chatbots and customer support.\n- **Real-time Data Processing**: Immediate analysis of streaming events.\n- **Multi-step Business Workflows**: Complex processes involving multiple AI components.\n\n### How Deployments Work\n- Requires a **Deployer** stack component to manage HTTP servers and lifecycle operations.\n- Deployments can be created using the ZenML CLI or SDK.\n\n### Deployment Lifecycle\n- Deployments have unique names, URLs, and statuses (RUNNING, ABSENT, PENDING, ERROR).\n- Managed independently of the active stack.\n\n### Best Practices\n1. Design for parameters with defaults.\n2. Return meaningful outputs.\n3. Use global initialization and cleanup hooks.\n4. Handle errors gracefully.\n\n### Conclusion\nPipeline deployments enable ZenML pipelines to function as real-time services, enhancing integration with web applications and real-time systems.",
    "content_quality_score": null,
    "child_urls": [
        "https://docs.zenml.io/",
        "https://zenml.io",
        "https://zenml.io/slack",
        "https://cloud.zenml.io/signup",
        "https://docs.zenml.io/user-guides",
        "https://docs.zenml.io/pro",
        "https://docs.zenml.io/stacks",
        "https://docs.zenml.io/api-reference",
        "https://docs.zenml.io/sdk-reference",
        "https://docs.zenml.io/changelog",
        "https://docs.zenml.io/getting-started/installation",
        "https://docs.zenml.io/getting-started/hello-world",
        "https://docs.zenml.io/getting-started/your-first-ai-pipeline",
        "https://docs.zenml.io/getting-started/core-concepts",
        "https://docs.zenml.io/getting-started/system-architectures",
        "https://docs.zenml.io/deploying-zenml/deploying-zenml",
        "https://docs.zenml.io/deploying-zenml/connecting-to-zenml",
        "https://docs.zenml.io/deploying-zenml/upgrade-zenml-server",
        "https://docs.zenml.io/concepts/steps_and_pipelines",
        "https://docs.zenml.io/concepts/artifacts",
        "https://docs.zenml.io/concepts/stack_components",
        "https://docs.zenml.io/concepts/service_connectors",
        "https://docs.zenml.io/concepts/snapshots",
        "https://docs.zenml.io/concepts/deployment",
        "https://docs.zenml.io/concepts/deployment/deployment_settings",
        "https://docs.zenml.io/concepts/containerization",
        "https://docs.zenml.io/concepts/code-repositories",
        "https://docs.zenml.io/concepts/secrets",
        "https://docs.zenml.io/concepts/environment-variables",
        "https://docs.zenml.io/concepts/tags",
        "https://docs.zenml.io/concepts/metadata",
        "https://docs.zenml.io/concepts/models",
        "https://docs.zenml.io/concepts/dashboard-features",
        "https://docs.zenml.io/concepts/templates",
        "https://docs.zenml.io/reference/community-and-content",
        "https://docs.zenml.io/reference/environment-variables",
        "https://docs.zenml.io/reference/llms-txt",
        "https://docs.zenml.io/reference/faq",
        "https://docs.zenml.io/reference/global-settings",
        "https://docs.zenml.io/reference/legacy-docs",
        "https://docs.zenml.io/concepts",
        "https://docs.zenml.io/stacks/stack-components/deployers",
        "https://github.com/zenml-io/zenml",
        "https://www.gitbook.com/",
        "https://github.com/zenml-io/zenml/tree/main/examples/agent_outer_loop",
        "https://github.com/zenml-io/zenml/tree/main/examples/deploying_agent",
        "https://github.com/zenml-io/zenml/blob/main/docs/book/component-guide/deployers/README.md"
    ]
}