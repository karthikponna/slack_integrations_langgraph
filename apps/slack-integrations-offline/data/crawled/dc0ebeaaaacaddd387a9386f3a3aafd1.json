{
    "id": "dc0ebeaaaacaddd387a9386f3a3aafd1",
    "metadata": {
        "id": "dc0ebeaaaacaddd387a9386f3a3aafd1",
        "url": "https://docs.zenml.io/deploying-zenml/upgrade-zenml-server/using-zenml-server-in-prod",
        "title": "Using ZenML server in production | ZenML - Bridging the gap between ML & Ops",
        "properties": {
            "description": "Learn about best practices for using ZenML server in production environments.",
            "keywords": null,
            "author": null,
            "og:title": "Using ZenML server in production | ZenML - Bridging the gap between ML & Ops",
            "og:description": "Learn about best practices for using ZenML server in production environments.",
            "og:image": "https://docs.zenml.io/~gitbook/ogimage/POfuH99d9qT2ryrW7DUE",
            "twitter:card": "summary_large_image",
            "twitter:title": "Using ZenML server in production | ZenML - Bridging the gap between ML & Ops",
            "twitter:description": "Learn about best practices for using ZenML server in production environments.",
            "twitter:image": "https://docs.zenml.io/~gitbook/ogimage/POfuH99d9qT2ryrW7DUE"
        }
    },
    "content": "`⌘``k`\nGitBook Assistant\nProductResourcesGitHubStart free\nMore\n  * Documentation\n  * Learn\n  * ZenML Pro\n  * Stacks\n  * API Reference\n  * SDK Reference\n  * Changelog\n\n\nGitBook Assistant\nGitBook Assistant\nWorking...Thinking...\nGitBook Assistant\n##### Good afternoon\nI'm here to help you with the docs.\nWhat is this page about?What should I read next?Can you give an example?\n`⌘``i`\nAI Based on your context\nSend\n  * Getting Started\n    *     *     *     *     *     *   * Deploying ZenML\n    *     *     *       * Best practices for upgrading\n      * Using ZenML server in production\n      * Troubleshoot your ZenML server\n      * Migration guide\n  * Concepts\n    *     *     *     *     *     *     *     *     *     *     *     *     *     *     *   * Reference\n    *     *     *     *     *     * \n\nPowered by GitBook\n  * Autoscaling replicas\n  * High connection pool values\n  * Scaling the backing database\n  * Setting up an ingress/load balancer\n  * Monitoring\n  * Backups\n\n\nWas this helpful?\nGitBook AssistantAsk\n  1. Deploying ZenML\n  2. \n\n# Using ZenML server in production\nLearn about best practices for using ZenML server in production environments.\nSetting up a ZenML server for testing is a quick process. However, most people have to move beyond so-called 'day zero' operations and in such cases, it helps to learn best practices around setting up your ZenML server in a production-ready way. This guide encapsulates all the tips and tricks we've learned ourselves and from working with people who use ZenML in production environments. Following are some of the best practices we recommend.\nIf you are using ZenML Pro, you don't have to worry about any of these. We have got you covered! You can sign up for a free trial here.\n## \nAutoscaling replicas\nIn production, you often have to run bigger and longer running pipelines that might strain your server's resources. It is a good idea to set up autoscaling for your ZenML server so that you don't have to worry about your pipeline runs getting interrupted or your Dashboard slowing down due to high traffic.\nHow you do it depends greatly on the environment in which you have deployed your ZenML server. Below are some common deployment options and how to set up autoscaling for them.\nKubernetes with Helm\nECS\nCloud Run\nDocker Compose\nIf you are using the official ZenML Helm chart, you can take advantage of the `autoscaling.enabled` flag to enable autoscaling for your ZenML server. For example:\nCopy```\nautoscaling:\nenabled:true\nminReplicas:1\nmaxReplicas:10\ntargetCPUUtilizationPercentage:80\n```\n\nThis will create a horizontal pod autoscaler for your ZenML server that will scale the number of replicas up to 10 and down to 1 based on the CPU utilization of the pods.\nFor folks using AWS, ECS is a popular choice for running ZenML server. ECS is a container orchestration service that allows you to run and scale your containers in a managed environment.\nTo scale your ZenML server deployed as a service on ECS, you can follow the steps below:\n  * Go to the ECS console, find you service pertaining to your ZenML server and click on it.\n  * Click on the \"Update Service\" button.\n  * If you scroll down, you will see the \"Service auto scaling - optional\" section.\n  * Here you can enable autoscaling and set the minimum and maximum number of tasks to run for your service and also the ECS service metric to use for scaling.\n\n\nImage showing autoscaling settings for a service\nFor folks on GCP, Cloud Run is a popular choice for running ZenML server. Cloud Run is a container orchestration service that allows you to run and scale your containers in a managed environment.\nIn Cloud Run, each revision is automatically scaled to the number of instances needed to handle all incoming requests, events, or CPU utilization and by default, when a revision does not receive any traffic, it is scaled in to zero instances. For production use cases, we recommend setting the minimum number of instances to at least 1 so that you have \"warm\" instances ready to serve incoming requests.\nTo scale your ZenML server deployed on Cloud Run, you can follow the steps below:\n  * Go to the Cloud Run console, find you service pertaining to your ZenML server and click on it.\n  * Click on the \"Edit & Deploy new Revision\" button.\n  * Scroll down to the \"Revision auto-scaling\" section.\n  * Here you can set the minimum and maximum number of instances to run for your service.\n\n\nImage showing autoscaling settings for a service\nIf you use Docker Compose, you don't get autoscaling out of the box. However, you can scale your service to N number of replicas using the `scale` flag. For example:\nCopy```\ndocker compose up --scale zenml-server=N\n```\n\nThis will scale your ZenML server to N replicas.\n## \nHigh connection pool values\nOne other way to improve the performance of your ZenML server is to increase the number of threads that your server process uses, provided that you have hardware that can support it.\nYou can control this by setting the `zenml.threadPoolSize` value in the ZenML Helm chart values. For example:\nCopy```\nzenml:\nthreadPoolSize:100\n```\n\nBy default, it is set to 40. If you are using any other deployment option, you can set the `ZENML_SERVER_THREAD_POOL_SIZE` environment variable to the desired value.\nOnce this is set, you should also modify the `zenml.database.poolSize` and `zenml.database.maxOverflow` values to ensure that the ZenML server workers do not block on database connections (i.e. the sum of the pool size and max overflow should be greater than or equal to the thread pool size). If you manage your own database, ensure these values are set appropriately.\n## \nScaling the backing database\nAn important component of the ZenML server deployment is the backing database. When you start scaling your ZenML server instances, you will also need to scale your database to avoid any bottlenecks.\nWe would recommend starting out with a simple (single) database instance and then monitoring it to decide if it needs scaling. Some common metrics to look out for:\n  * CPU Utilization: If the CPU Utilization is consistently above 50%, you may need to scale your database. Some spikes in the utlization are expected but it should not be consistently high.\n  * Freeable Memory: It is natural for the freeable memory to go down with time as your database uses it for caching and buffering but if it drops below 100-200 MB, you may need to scale your database.\n\n\n## \nSetting up an ingress/load balancer\nExposing your ZenML server to the internet securely and reliably is a must for production use cases. One way to do this is to set up an ingress/load balancer.\nKubernetes with Helm\nECS\nCloud Run\nDocker Compose\nIf you are using the official ZenML Helm chart, you can take advantage of the `zenml.ingress.enabled` flag to enable ingress for your ZenML server. For example:\nCopy```\nzenml:\n  ingress:\n    enabled: true\n    className: \"nginx\"\n    annotations:\n      # nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n      # nginx.ingress.kubernetes.io/rewrite-target: /$1\n      # kubernetes.io/ingress.class: nginx\n      # kubernetes.io/tls-acme: \"true\"\n      # cert-manager.io/cluster-issuer: \"letsencrypt\"\n```\n\nThis will create an NGINX ingress for your ZenML service that will create a LoadBalancer on whatever cloud provider you are using.\nWith ECS, you can use Application Load Balancers to evenly route traffic to your tasks running your ZenML server.\nFollow the steps in the official AWS documentation to learn how to set this up.\nWith Cloud Run, you can use Cloud Load Balancing to route traffic to your service.\nFollow the steps in the official GCP documentation to learn how to set this up.\nIf you are using Docker Compose, you can set up an NGINX server as a reverse proxy to route traffic to your ZenML server. Here's a blog that shows how to do it.\n## \nMonitoring\nMonitoring your service is crucial to ensure that it is running smoothly and to catch any issues early before they can cause problems. Depending on the deployment option you are using, you can use different tools to monitor your service.\nKubernetes with Helm\nECS\nCloud Run\nYou can set up Prometheus and Grafana to monitor your ZenML server. We recommend using the `kube-prometheus-stack` Helm chart from the prometheus-community to get started quickly.\nOnce you have deployed the chart, you can find your grafana service by searching for services in the namespace you have deployed the chart in. Port-forward it to your local machine or deploy it through an ingress.\nYou can now use queries like the following to monitor your ZenML server:\nCopy```\nsum by(namespace) (rate(container_cpu_usage_seconds_total{namespace=~\"zenml.*\"}[5m]))\n```\n\nThis query would give you the CPU utilization of your server pods in all namespaces that start with `zenml`. The image below shows how this query would look like in Grafana.\nImage showing CPU utilization of ZenML server pods\nOn ECS, you can utilize the CloudWatch integration to monitor your ZenML server.\nIn the \"Health and metrics\" section of your ECS console, you should see metrics pertaining to your ZenML service like CPU utilization and Memory utilization.\nImage showing CPU utilization ECS\nIn Cloud Run, you can utilize the Cloud Monitoring integration to monitor your ZenML server.\nThe \"Metrics\" tab in the Cloud Run console will show you metrics like Container CPU utilization, Container memory utilization, and more.\nImage showing metrics in Cloud Run\n## \nBackups\nThe data in your ZenML server is critical as it contains your pipeline runs, stack configurations, and other important information. It is, therefore, recommended to have a backup strategy in place to avoid losing any data.\nSome common strategies include:\n  * Setting up automated backups with a good retention period (say 30 days).\n  * Periodically exporting the data to an external storage (e.g. S3, GCS, etc.).\n  * Manual backups before upgrading your server to avoid any problems.\n\n\nPreviousBest practices for upgradingNextTroubleshoot your ZenML server\nLast updated 4 months ago\nWas this helpful?\nThis site uses cookies to deliver its service and to analyze traffic. By browsing this site, you accept the privacy policy.\nAcceptReject\n",
    "summary": null,
    "content_quality_score": null,
    "child_urls": [
        "https://docs.zenml.io/",
        "https://zenml.io",
        "https://zenml.io/slack",
        "https://cloud.zenml.io/signup",
        "https://docs.zenml.io/user-guides",
        "https://docs.zenml.io/pro",
        "https://docs.zenml.io/stacks",
        "https://docs.zenml.io/api-reference",
        "https://docs.zenml.io/sdk-reference",
        "https://docs.zenml.io/changelog",
        "https://docs.zenml.io/getting-started/installation",
        "https://docs.zenml.io/getting-started/hello-world",
        "https://docs.zenml.io/getting-started/your-first-ai-pipeline",
        "https://docs.zenml.io/getting-started/core-concepts",
        "https://docs.zenml.io/getting-started/system-architectures",
        "https://docs.zenml.io/deploying-zenml/deploying-zenml",
        "https://docs.zenml.io/deploying-zenml/connecting-to-zenml",
        "https://docs.zenml.io/deploying-zenml/upgrade-zenml-server",
        "https://docs.zenml.io/deploying-zenml/upgrade-zenml-server/best-practices-upgrading-zenml",
        "https://docs.zenml.io/deploying-zenml/upgrade-zenml-server/using-zenml-server-in-prod",
        "https://docs.zenml.io/deploying-zenml/upgrade-zenml-server/troubleshoot-your-deployed-server",
        "https://docs.zenml.io/deploying-zenml/upgrade-zenml-server/migration-guide",
        "https://docs.zenml.io/concepts/steps_and_pipelines",
        "https://docs.zenml.io/concepts/artifacts",
        "https://docs.zenml.io/concepts/stack_components",
        "https://docs.zenml.io/concepts/service_connectors",
        "https://docs.zenml.io/concepts/snapshots",
        "https://docs.zenml.io/concepts/deployment",
        "https://docs.zenml.io/concepts/containerization",
        "https://docs.zenml.io/concepts/code-repositories",
        "https://docs.zenml.io/concepts/secrets",
        "https://docs.zenml.io/concepts/environment-variables",
        "https://docs.zenml.io/concepts/tags",
        "https://docs.zenml.io/concepts/metadata",
        "https://docs.zenml.io/concepts/models",
        "https://docs.zenml.io/concepts/dashboard-features",
        "https://docs.zenml.io/concepts/templates",
        "https://docs.zenml.io/reference/community-and-content",
        "https://docs.zenml.io/reference/environment-variables",
        "https://docs.zenml.io/reference/llms-txt",
        "https://docs.zenml.io/reference/faq",
        "https://docs.zenml.io/reference/global-settings",
        "https://docs.zenml.io/reference/legacy-docs",
        "https://docs.zenml.io/deploying-zenml",
        "https://zenml.io/pro",
        "https://www.zenml.io/privacy-policy",
        "https://github.com/zenml-io/zenml",
        "https://www.gitbook.com/",
        "https://artifacthub.io/packages/helm/zenml/zenml",
        "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html",
        "https://cloud.google.com/run",
        "https://github.com/kubernetes/ingress-nginx",
        "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html",
        "https://cloud.google.com/load-balancing/docs/https/setting-up-https-serverless",
        "https://www.docker.com/blog/how-to-use-the-official-nginx-docker-image",
        "https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack",
        "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cloudwatch-metrics.html",
        "https://cloud.google.com/run/docs/monitoring"
    ]
}